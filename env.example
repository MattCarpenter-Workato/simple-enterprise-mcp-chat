# OpenAI API Key
OPENAI_API_KEY=your_openai_api_key_here

# Model to use (default: gpt-4o-mini)
MODEL=gpt-4o-mini

# LM Studio Configuration (for chat-lmstudio.py)
# Base URL for LM Studio server (default: http://localhost:1234/v1)
LMSTUDIO_BASE_URL=http://localhost:1234/v1

# Model name (LM Studio typically ignores this and uses the loaded model)
LMSTUDIO_MODEL=local-model

# Optional: System prompt to guide LLM behavior (works with both chat.py and chat-lmstudio.py)
# Examples:
# SYSTEM_PROMPT=You are a helpful medical assistant.
# SYSTEM_PROMPT=You are a concise assistant that answers in bullet points.
SYSTEM_PROMPT=

# Logging Configuration
# LOG_LEVEL: DEBUG, INFO, WARNING, ERROR, CRITICAL
# DEBUG: Shows detailed MCP server requests/responses and OpenAI API calls
# INFO: Shows high-level operations and tool calls (recommended)
# WARNING: Shows only warnings and errors
LOG_LEVEL=INFO

# LOG_FILE: Path to log file (leave empty for no file logging)
# Example: logs/chat.log
# The directory will be created automatically if it doesn't exist
LOG_FILE=

# LOG_TO_CONSOLE: Whether to output logs to terminal (true/false)
# true: Show logs in terminal (and file if LOG_FILE is set)
# false: Only log to file, keeps terminal clean
LOG_TO_CONSOLE=true

