# =============================================================================
# OpenAI-Compatible Models Configuration (OpenAI, LM Studio)
# =============================================================================

# OpenAI API Key (from https://platform.openai.com/)
OPENAI_API_KEY=your_openai_api_key_here

# Model to use (default: gpt-4o-mini)
# Options: gpt-4o, gpt-4o-mini, gpt-3.5-turbo
# Used by chat-openai.py
MODEL=gpt-4o-mini

# =============================================================================
# Claude API Configuration (Anthropic)
# =============================================================================

# Claude API Key (from https://console.anthropic.com/)
CLAUDE_API_KEY=your_claude_api_key_here

# Claude model to use (default: claude-3-5-sonnet-20241022)
# Options: claude-3-5-sonnet-20241022, claude-3-opus-20240229, claude-3-sonnet-20240229, claude-3-haiku-20240307
# Used by chat-claude.py
CLAUDE_MODEL=claude-3-5-sonnet-20241022

# =============================================================================
# LM Studio Configuration (for chat-lmstudio.py)
# =============================================================================

# Base URL for LM Studio server (default: http://localhost:1234/v1)
LMSTUDIO_BASE_URL=http://localhost:1234/v1

# Model name (LM Studio typically ignores this and uses the loaded model)
LMSTUDIO_MODEL=local-model

# =============================================================================
# General Configuration (Applies to all chat scripts)
# =============================================================================

# Automatically inject current date/time into each user message (true/false)
# This helps the LLM know the current date for time-based queries
# Recommended: true (especially for date-sensitive applications like CGM data)
INJECT_CURRENT_DATE=true

# System Prompt (used by all chat scripts if not overridden by command line argument)
# Examples:
# SYSTEM_PROMPT=You are a helpful medical assistant.
# SYSTEM_PROMPT=You are a concise assistant that answers in bullet points.
SYSTEM_PROMPT=

# =============================================================================
# Logging Configuration (Applies to all chat scripts)
# =============================================================================

# LOG_LEVEL: DEBUG, INFO, WARNING, ERROR, CRITICAL
# DEBUG: Shows detailed MCP server requests/responses and API calls
# INFO: Shows high-level operations and tool calls (recommended)
# WARNING: Shows only warnings and errors
LOG_LEVEL=INFO

# LOG_FILE: Path to log file (leave empty for no file logging)
# Example: logs/chat.log
# The directory will be created automatically if it doesn't exist
LOG_FILE=

# LOG_TO_CONSOLE: Whether to output logs to terminal (true/false)
# true: Show logs in terminal (and file if LOG_FILE is set)
# false: Only log to file, keeps terminal clean
LOG_TO_CONSOLE=true

# TOKEN_LOG_FILE: Separate log file for token usage tracking (leave empty to disable)
# Tracks all API calls with token counts for cost monitoring
# Example: logs/tokens.log
TOKEN_LOG_FILE=

